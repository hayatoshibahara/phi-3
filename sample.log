🟩 Transformers version: 4.57.1
🟩 Numpy version: 2.0.2
🟩 BitsAndBytes version: 0.48.1
🟩 loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/tokenizer.model
🟩 loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/tokenizer.json
🟩 loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/added_tokens.json
🟩 loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/special_tokens_map.json
🟩 loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/tokenizer_config.json
🟩 loading file chat_template.jinja from cache at None
🟩 Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
🟨 The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
🟩 loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/config.json
🟨 Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'short_mscale', 'long_mscale'}
🟨 This model has set a `original_max_position_embeddings` field, to be used together with `max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_scaling`with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, as it is compatible with most model architectures.
🟩 Model config PhimoeConfig {
  "architectures": [
    "PhiMoEForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phimoe.PhiMoEConfig",
    "AutoModelForCausalLM": "modeling_phimoe.PhiMoEForCausalLM"
  },
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "input_jitter_noise": 0.01,
  "intermediate_size": 6400,
  "lm_head_bias": true,
  "max_position_embeddings": 131072,
  "model_type": "phimoe",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 16,
  "original_max_position_embeddings": 4096,
  "output_router_logits": false,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "long_factor": [
      1.0199999809265137,
      1.0299999713897705,
      1.0399999618530273,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.059999942779541,
      1.059999942779541,
      1.059999942779541,
      1.059999942779541,
      1.059999942779541,
      1.059999942779541,
      1.0999999046325684,
      1.1799999475479126,
      1.1799999475479126,
      1.3700000047683716,
      1.4899998903274536,
      2.109999895095825,
      2.8899998664855957,
      3.9499998092651367,
      4.299999713897705,
      6.429999828338623,
      8.09000015258789,
      10.690000534057617,
      12.050000190734863,
      18.229999542236328,
      18.84000015258789,
      19.899999618530273,
      21.420000076293945,
      26.200000762939453,
      34.28000259399414,
      34.590003967285156,
      38.730003356933594,
      40.22000503540039,
      42.54000473022461,
      44.000003814697266,
      47.590003967285156,
      54.750003814697266,
      56.19000244140625,
      57.44000244140625,
      57.4900016784668,
      61.20000076293945,
      61.540000915527344,
      61.75,
      61.779998779296875,
      62.06999969482422,
      63.11000061035156,
      63.43000030517578,
      63.560001373291016,
      63.71000289916992,
      63.92000198364258,
      63.94000244140625,
      63.94000244140625,
      63.96000289916992,
      63.980003356933594,
      64.0300064086914,
      64.0300064086914,
      64.0300064086914,
      64.04000854492188,
      64.10000610351562,
      64.19000244140625,
      64.20999908447266,
      64.75,
      64.95999908447266
    ],
    "long_mscale": 1.243163121016122,
    "original_max_position_embeddings": 4096,
    "rope_type": "longrope",
    "short_factor": [
      1.0,
      1.0399999618530273,
      1.0399999618530273,
      1.0399999618530273,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.0499999523162842,
      1.059999942779541,
      1.059999942779541,
      1.0699999332427979,
      1.0699999332427979,
      1.0699999332427979,
      1.0699999332427979,
      1.1399999856948853,
      1.159999966621399,
      1.159999966621399,
      1.159999966621399,
      1.159999966621399,
      1.1799999475479126,
      1.1999999284744263,
      1.3199999332427979,
      1.3399999141693115,
      1.3499999046325684,
      1.3999998569488525,
      1.4799998998641968,
      1.4999998807907104,
      1.589999794960022,
      1.6499998569488525,
      1.71999990940094,
      1.8999998569488525,
      1.9099998474121094,
      1.9099998474121094,
      1.9899998903274536,
      1.9999998807907104,
      1.9999998807907104,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.009999990463257,
      2.0999999046325684,
      2.319999933242798,
      2.419999837875366,
      2.5899999141693115,
      2.7899999618530273
    ],
    "short_mscale": 1.243163121016122,
    "type": "longrope"
  },
  "rope_theta": 10000.0,
  "router_aux_loss_coef": 0.0,
  "router_jitter_noise": 0.01,
  "sliding_window": 131072,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 32064
}

🟦 Multi-backend validation successful.
🟩 loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/model.safetensors.index.json
🟩 Instantiating PhimoeForCausalLM model under default dtype torch.bfloat16.
🟩 PhimoeForCausalLMを初期化開始 config.vocab_size=32064 config.hidden_size=4096 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_aux_loss_coef=0.0
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000
}

🟩 PhimoeModelを初期化開始 config.vocab_size=32064 config.hidden_size=4096 config.num_hidden_layers=32 config._attn_implementation='sdpa'
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=0
🟩 PhimoeAttentionを初期化開始 layer_idx=0 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=1
🟩 PhimoeAttentionを初期化開始 layer_idx=1 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=2
🟩 PhimoeAttentionを初期化開始 layer_idx=2 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=3
🟩 PhimoeAttentionを初期化開始 layer_idx=3 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=4
🟩 PhimoeAttentionを初期化開始 layer_idx=4 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=5
🟩 PhimoeAttentionを初期化開始 layer_idx=5 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=6
🟩 PhimoeAttentionを初期化開始 layer_idx=6 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=7
🟩 PhimoeAttentionを初期化開始 layer_idx=7 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=8
🟩 PhimoeAttentionを初期化開始 layer_idx=8 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=9
🟩 PhimoeAttentionを初期化開始 layer_idx=9 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=10
🟩 PhimoeAttentionを初期化開始 layer_idx=10 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=11
🟩 PhimoeAttentionを初期化開始 layer_idx=11 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=12
🟩 PhimoeAttentionを初期化開始 layer_idx=12 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=13
🟩 PhimoeAttentionを初期化開始 layer_idx=13 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=14
🟩 PhimoeAttentionを初期化開始 layer_idx=14 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=15
🟩 PhimoeAttentionを初期化開始 layer_idx=15 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=16
🟩 PhimoeAttentionを初期化開始 layer_idx=16 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=17
🟩 PhimoeAttentionを初期化開始 layer_idx=17 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=18
🟩 PhimoeAttentionを初期化開始 layer_idx=18 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=19
🟩 PhimoeAttentionを初期化開始 layer_idx=19 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=20
🟩 PhimoeAttentionを初期化開始 layer_idx=20 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=21
🟩 PhimoeAttentionを初期化開始 layer_idx=21 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=22
🟩 PhimoeAttentionを初期化開始 layer_idx=22 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=23
🟩 PhimoeAttentionを初期化開始 layer_idx=23 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=24
🟩 PhimoeAttentionを初期化開始 layer_idx=24 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=25
🟩 PhimoeAttentionを初期化開始 layer_idx=25 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=26
🟩 PhimoeAttentionを初期化開始 layer_idx=26 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=27
🟩 PhimoeAttentionを初期化開始 layer_idx=27 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=28
🟩 PhimoeAttentionを初期化開始 layer_idx=28 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=29
🟩 PhimoeAttentionを初期化開始 layer_idx=29 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=30
🟩 PhimoeAttentionを初期化開始 layer_idx=30 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeDecoderLayerを初期化開始 config.hidden_size=4096 config._attn_implementation='sdpa' layer_idx=31
🟩 PhimoeAttentionを初期化開始 layer_idx=31 config.hidden_size=4096 config.num_attention_heads=32 config.num_key_value_heads=8 config.max_position_embeddings=131072 config.rope_theta=10000.0 config.attention_dropout=0.0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟩 PhimoeAttentionを初期化完了
🟦 self.self_attn=PhimoeSdpaAttention(
  (q_proj): Linear(in_features=4096, out_features=4096, bias=True)
  (k_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (v_proj): Linear(in_features=4096, out_features=1024, bias=True)
  (o_proj): Linear(in_features=4096, out_features=4096, bias=True)
)
🟩 PhimoeSparseMoeBlockを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.num_local_experts=16 config.num_experts_per_tok=2 config.router_jitter_noise=0.01 config.input_jitter_noise=0.01
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeBlockSparseTop2MLPを初期化開始 config.hidden_size=4096 config.intermediate_size=6400 config.hidden_act='silu'
🟦 self.act_fn=SiLUActivation()
🟩 PhimoeBlockSparseTop2MLPを初期化完了
🟩 PhimoeSparseMoeBlockを初期化完了
🟩 PhimoeDecoderLayerを初期化完了
🟩 PhimoeRotaryEmbeddingを初期化開始 config.rope_scaling={'long_factor': [1.0199999809265137, 1.0299999713897705, 1.0399999618530273, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.059999942779541, 1.059999942779541, 1.059999942779541, 1.059999942779541, 1.059999942779541, 1.059999942779541, 1.0999999046325684, 1.1799999475479126, 1.1799999475479126, 1.3700000047683716, 1.4899998903274536, 2.109999895095825, 2.8899998664855957, 3.9499998092651367, 4.299999713897705, 6.429999828338623, 8.09000015258789, 10.690000534057617, 12.050000190734863, 18.229999542236328, 18.84000015258789, 19.899999618530273, 21.420000076293945, 26.200000762939453, 34.28000259399414, 34.590003967285156, 38.730003356933594, 40.22000503540039, 42.54000473022461, 44.000003814697266, 47.590003967285156, 54.750003814697266, 56.19000244140625, 57.44000244140625, 57.4900016784668, 61.20000076293945, 61.540000915527344, 61.75, 61.779998779296875, 62.06999969482422, 63.11000061035156, 63.43000030517578, 63.560001373291016, 63.71000289916992, 63.92000198364258, 63.94000244140625, 63.94000244140625, 63.96000289916992, 63.980003356933594, 64.0300064086914, 64.0300064086914, 64.0300064086914, 64.04000854492188, 64.10000610351562, 64.19000244140625, 64.20999908447266, 64.75, 64.95999908447266], 'long_mscale': 1.243163121016122, 'original_max_position_embeddings': 4096, 'short_factor': [1.0, 1.0399999618530273, 1.0399999618530273, 1.0399999618530273, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.0499999523162842, 1.059999942779541, 1.059999942779541, 1.0699999332427979, 1.0699999332427979, 1.0699999332427979, 1.0699999332427979, 1.1399999856948853, 1.159999966621399, 1.159999966621399, 1.159999966621399, 1.159999966621399, 1.1799999475479126, 1.1999999284744263, 1.3199999332427979, 1.3399999141693115, 1.3499999046325684, 1.3999998569488525, 1.4799998998641968, 1.4999998807907104, 1.589999794960022, 1.6499998569488525, 1.71999990940094, 1.8999998569488525, 1.9099998474121094, 1.9099998474121094, 1.9899998903274536, 1.9999998807907104, 1.9999998807907104, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.009999990463257, 2.0999999046325684, 2.319999933242798, 2.419999837875366, 2.5899999141693115, 2.7899999618530273], 'short_mscale': 1.243163121016122, 'type': 'longrope', 'rope_type': 'longrope'}
🟦 self.short_mscale=1.243163121016122
🟦 self.long_mscale=1.243163121016122
🟦 self.rope_init_fn=<function _compute_longrope_parameters at 0x7eb080193e20>
🟩 PhimoeRotaryEmbeddingを初期化完了
🟩 PhimoeModelを初期化完了
🟩 PhimoeForCausalLMを初期化完了
🟩 loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3.5-MoE-instruct/snapshots/c5ec1449e5376ad4c7031bf0d51eabf5e7d08887/generation_config.json
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": [
    32000,
    32001,
    32007
  ],
  "pad_token_id": 32000
}

🟩 Could not locate the custom_generate/generate.py inside microsoft/Phi-3.5-MoE-instruct.
🟨 Device set to use cuda
🟨 The following generation flags are not valid and may be ignored: ['temperature'].
🟩 - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
🟩 PhimoeForCausalLMの順伝播開始 input_ids.shape if input_ids is not None else None=torch.Size([1, 130]) attention_mask.shape if attention_mask is not None else None=torch.Size([1, 130]) position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True inputs_embeds.shape if inputs_embeds is not None else None=None labels.shape if labels is not None else None=None use_cache=True output_attentions=None output_hidden_states=None output_router_logits=None cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') logits_to_keep=1
🟩 PhimoeModelの順伝播開始 input_ids.shape if input_ids is not None else None=torch.Size([1, 130]) attention_mask.shape if attention_mask is not None else None=torch.Size([1, 130]) position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True inputs_embeds.shape if inputs_embeds is not None else None=None use_cache=True output_attentions=False output_hidden_states=False output_router_logits=False cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0')
🟦 inputs_embeds.shape=torch.Size([1, 130, 4096])
🟩 PhimoeRotaryEmbeddingの順伝播開始 x.shape=torch.Size([1, 130, 4096]) seq_len=tensor(130, device='cuda:0')
🟦 self.config.rope_scaling['original_max_position_embeddings']=4096
🟦 mscale=1.243163121016122
🟦 inv_freq.shape=torch.Size([64])
🟦 mscale=1.243163121016122
🟦 t.shape=torch.Size([130])
🟦 freqs.shape=torch.Size([130, 64])
🟩 PhimoeRotaryEmbeddingの順伝播完了 res[0].shape=torch.Size([130, 128]) res[1].shape=torch.Size([130, 128])
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 72, 107, 118, 121, 122, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0], device='cuda:0'), top_x=tensor([  3,  15,  35, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([ 47,  77, 118, 119, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  4,  19,  22,  37,  40,  52,  56,  61,  64,  82,  89,  91,  93,  96,
        104, 115,  17,  25,  27,  43,  49,  55,  79,  85], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  14,  34,  60,  92, 106,  16,  36, 115, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  49,  59,  73,  79,  87,  90, 105, 108,  19,  29,  37,  61,  86,
         91,  93, 104], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 45,  46,  47,  48,  75, 110, 117, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([  8,  10,  26,  28,  30,  45,  48,  58,  75,  78,  88, 110, 112, 120,
        123, 127, 129,   9], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,  12,  29,  31, 113,   4,  33,  34,  50,  70,  74,  80, 102, 103,
        109, 111, 128, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([ 77,  78, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  46,  51,  63,  69,  72,  76,  81,  95, 101, 107, 117, 121,
        124,   2,  18,  32, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,   5,   6,  11,  16,  32,  33,  36,  44,  68,  70,  86, 100, 102,
        103,   7,  10,  12,  13,  14,  22,  31,  40,  52,  56,  60,  64,  71,
         82,  96, 105, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 18,  23,  41,  53,  57,  65,  83,  97, 114,  20,  24,  38,  42,  44,
         54,  62,  66,  68,  84,  87,  94,  98, 100, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  24,  27,  42,  54,  66,  84,  98, 111, 128,   0,   1,  11,  23,
         26,  41,  53,  57,  59,  65,  67,  73,  83,  90,  92,  97,  99, 108,
        112, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([30, 4096])
🟦 gate.shape=torch.Size([30, 6400])
🟦 up.shape=torch.Size([30, 6400])
🟦 current_hidden_states.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([30, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
       device='cuda:0'), top_x=tensor([  7,  17,  20,  25,  38,  43,  50,  55,  62,  67,  71,  74,  80,  85,
         94,  99, 109, 126,  28,  89], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  3,   5,   6,   8,  15,  21,  30,  35,  39,  51,  58,  63,  69,  76,
         81,  88,  95, 101, 116, 120, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   8,  33,  46,  69,  70,  76, 101, 102, 116, 117,   2,   5,  21,
         39,  75,  85, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 22,  40,  52,  60,  64,  82,  90,  96,  16,  29,  31,  33,  36,  57,
         70,  73,  86,  89, 102, 108, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18, 114,  15,  32,  35,  44,  68,  69, 100, 101], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 23,  41,  53,  65,  83,  91,  97, 109,  20,  38,  50,  62,  74,  80,
         94, 104], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 1,  2,  6, 11, 12, 15, 32, 35, 13], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([ 14,  16,  20,  31,  34,  36,  38,  44,  50,  62,  68,  71,  74,  80,
         94, 100, 106,   7,  25,  43,  56,  59,  67,  99, 103, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  29, 126,   6,  14,  34,  47,  77, 106, 118, 119, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  27, 111, 120, 123, 124, 128,  48,  78, 110, 121],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([113, 119,  26,  30,  45,  51,  58,  63,  72,  81,  88,  95, 107, 127],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  57,  63,  72,  81,  95, 103, 107,   0,  18, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  19,  37,  56,  61,  93, 115,  17,  22,  40,  49,  52,  55,  64,
         79,  82,  96, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  92,   1,   4,  12,  24,  42,  54,  60,  66,  71,  84,  87,  90,
         98, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,  10,  26,  28,  30,  45,  48,  58,  75,  78,  88, 110, 112, 127,
        129,   8,   9,  27, 111, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  25,  43,  49,  55,  59,  67,  79,  85,  86,  87,  89,  99, 104,
        105,  11,  19,  23,  37,  41,  53,  61,  65,  83,  91,  92,  93,  97],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,  24,  42,  47,  54,  66,  73,  77,  84,  98, 108, 118, 121, 122,
        125,   3,  46,  76, 117, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 10,  28, 112, 120, 123, 126, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([120], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,  12,  31,  33,  87,   1,  24,  42,  49,  54,  66,  70,  79,  84,
         86,  98, 102, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  25,  34,  43,  67,  99,  17,  48,  75,  78,  85, 110],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 45,  48,  58,  75,  78,  88, 110,  26,  28,  30, 112, 121, 127],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 120,  23,  41,  53,  55,  65,  80,  83,  89,  97, 123, 124, 126],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  72,  81,  95, 107,   3,  46,  60],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  2,   9,  10,  11,  26,  27,  30,  32,  70, 102, 111, 112, 113, 127,
        128,  13,  14,  33,  36,  44,  68, 100, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  74, 106, 109, 126,  20,  34,  38,  50,  57,  62,  71,  94],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,  20,  38,  50,  57,  62,  73,  80,  94, 108,   2,   6,   8,   9,
         27,  47,  77,  87, 103, 111, 118, 122, 125, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  24,  42,  54,  66,  84,  90,  98,  40,  52,  64,  73, 119],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 47,  76,  77, 118, 121, 122, 124, 125, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([  7,  40,  52,  55,  56,  64,  71,  82,  85,  89,  91,  96,   4,  19,
         22,  25,  37,  43,  59,  61,  67,  74,  93,  99, 104, 105, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  13,  16,  29,  36,  60,  92, 115,  69, 101, 114],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  0,  19,  37,  49,  59,  61,  79,  86,  93, 104, 105, 116,  15,  21,
         35,  39,  51,  56,  63,  72,  81,  82,  95,  96, 107],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 15,  18,  23,  35,  41,  44,  53,  65,  68,  69,  83,  97, 100, 101,
        114,  16,  91,  92, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([  1,   3,   8,  28,  46, 103, 117, 123, 129,   0,   5,   7,  10,  11,
         12,  18,  29,  31,  32,  45,  58,  76,  88,  90, 106, 113, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([111, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  30,  34,  75, 110,   0,   7,  26,  33,  45,  48,  70, 102, 127],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  7,  25,  43,  55,  57,  62,  67,  71,  85,  87,  99,  20,  23,  38,
         41,  50,  53,  65,  80,  83,  94,  97, 103, 106, 126],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   5,   8,  46, 106, 116, 126,  17,  44,  68,  76, 100, 113, 117],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 23,  41,  53,  65,  83,  97, 104, 125,   8,  16,  29,  46,  47,  57,
         77,  89, 118, 122, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 44,  68,  69, 100, 101,  15,  27,  28,  31,  35,  58,  63,  81, 120,
        123], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([  1,   2,  11,  12,  29,  31,  32,  33,  70, 102, 113,  34],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,  10,  15,  18,  26,  35,  45,  48,  58,  78,  88, 112, 114, 127,
         11,  12,  30,  75, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  47,  51,  63,  72,  77,  81,  95, 107, 118, 122,   1,   2,
          9,  14,  32,  67,  78,  99, 124, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([  4,  17,  60,  89, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 20,  24,  38,  42,  49,  50,  54,  59,  66,  73,  74,  79,  80,  84,
         86,  90,  94,  98, 103, 105, 108, 109,   6,  19,  25,  37,  56,  61,
         62,  93, 104], device='cuda:0')
🟦 current_state.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([31, 4096])
🟦 gate.shape=torch.Size([31, 6400])
🟦 up.shape=torch.Size([31, 6400])
🟦 current_hidden_states.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([31, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,  43,  52,  64,  82,  91,  96, 116, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  27,  28,  76, 111, 117, 119, 120, 121, 123, 124, 128, 129,  10,
         13,  88, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  16,  36,  18,  21,  39,  51,  60,  69,  72,  92,  95, 101, 107,
        114, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,   5,  22,  24,  40,  42,  54,  66,  73,  84,  90,  98, 108, 119],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  37,  40,  52,  56,  61,  64,  82,  91,  92,  93,  96,   4,
         36,  49,  55,  59,  71,  74,  79,  85,  86,  87, 105, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  41,  53,  62,  65,  83,  87,  97, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([  8,  26,  30,  45,  48,  58,  75,  76,  78,  88, 110, 112, 113, 127,
         10,  12,  28,  33,  70,  77, 102, 111, 117, 118, 128, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  10,  27,  28,  46, 111, 121, 128, 129,   0,  11, 112, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 47,  77, 118, 119, 120, 122, 123, 124, 125,  78], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  19,  23,  24,  37,  42,  49,  53,  54,  61,  65,  66,  73,  79,
         83,  84,  93,  97,  98, 103, 108,   1,   9,  27,  41,  50,  59,  74,
         80,  86,  90,  94, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([33, 4096])
🟦 gate.shape=torch.Size([33, 6400])
🟦 up.shape=torch.Size([33, 6400])
🟦 current_hidden_states.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([33, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,   7,  17, 126,  22,  52,  64,  82,  91,  96, 115],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 14,  31,  34,  44,  68, 100,  72], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([ 20,  25,  38,  43,  50,  55,  57,  62,  67,  71,  74,  80,  85,  86,
         87,  94,  99, 105, 106, 109,   6,   7,  16,  17,  36,  60, 104],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,  18,  69, 101,  15,  31,  32,  35], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([117,   2,   3,   8,  21,  26,  30,  39,  44,  45,  48,  51,  58,  63,
         75,  76,  81,  88,  95, 107, 110, 114, 124, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  13,  16,  29,  36,  59,  60,  89,  91, 115,  40,  56,  92],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  12,  15,  35,   4,  13,  14,  34,  46,  68, 100, 113, 121],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 13/16 の処理スキップ
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 56,  92, 104,  20,  25,  38,  57], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  40,  52,  64,  82,  90,  96,   5,  19,  23,  24,  29,  37,  42,
         43,  47,  49,  54,  55,  61,  66,  67,  71,  73,  79,  84,  85,  89,
         93,  98,  99, 103, 105, 106, 108, 119, 120, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([39, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([39, 4096])
🟦 gate.shape=torch.Size([39, 6400])
🟦 up.shape=torch.Size([39, 6400])
🟦 current_hidden_states.shape=torch.Size([39, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([39, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  2,   3,  21,  32,  33,  39,  51,  63,  70,  72,  81,  95, 102, 107,
        114, 116,  18,  69, 101, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  35, 122,  47,  77, 118, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  17,  20,  34,  38,  62,  94, 106,  25,  43,  67,  71,  90,  99,
        126], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 23,  25,  41,  43,  53,  55,  65,  67,  73,  74,  83,  85,  86,  87,
         90,  97,  99, 103, 104, 105, 108, 109,  19,  37,  57,  59,  61,  93],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 1], device='cuda:0'), top_x=tensor([125, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  69,  72,  81,  95, 101, 107,  44,  45,  46,  58,
         68,  88, 100, 121, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  56,  16,  29,  36,  60,  92, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,   1,   2,   5,  11,  12,  29,  31,  32,  47,  49,  59,  77,  79,
        113, 118,   3,   6,   7,   9,  10,  13,  28,  48,  76,  78,  89, 111,
        112, 114, 120, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,  33,  70, 102, 114, 116,   0,   2,  12,  14,  15,  31,  32,  34,
         35, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 120, 123, 124,  72, 107, 122], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  57, 126,  17,  87,  91, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1], device='cuda:0'), top_x=tensor([  8,   9,  10,  26,  27,  28,  30,  45,  46,  48,  58,  75,  76,  78,
         88, 110, 111, 112, 117, 121, 127, 128, 129,   1,  11,  39],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([21, 24, 50, 51, 63, 80, 81, 95, 98], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 13,  16,  36,  40,  44,  60,  68,  89,  91,  92, 100, 115,   4,  18,
         22,  52,  56,  64,  69,  96, 101], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,  24,  42,  50,  54,  66,  71,  80,  84,  98,   5,  20,  23,  38,
         62,  73,  74,  82,  94, 104, 106, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,   8,  26,  27,  30,  33,  70,  75, 102, 110, 116, 117, 124, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  37,  52,  61,  64,  82,  93,  96,  40,  41,  42,  49,  53,
         54,  55,  65,  66,  79,  83,  84,  85,  86,  97, 103, 105, 108],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  20,  22,  23,  24,  25,  37,  38,  40,  41,  42,  43,  49,  50,
         52,  53,  54,  55,  56,  57,  59,  60,  61,  62,  64,  65,  66,  67,
         71,  73,  74,  79,  80,  82,  83,  84,  85,  86,  87,  89,  90,  91,
         92,  93,  94,  96,  97,  98,  99, 103, 104, 105, 106, 108, 109,  16,
         21,  36,  68], device='cuda:0')
🟦 current_state.shape=torch.Size([59, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([59, 4096])
🟦 gate.shape=torch.Size([59, 6400])
🟦 up.shape=torch.Size([59, 6400])
🟦 current_hidden_states.shape=torch.Size([59, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([59, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,  12,  13,  29,  31,  32,  33,  70, 102, 113, 114,  11,  34,  44],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([20, 22, 49, 59, 79], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,   6,   7,   9,  27,  46,  68, 100, 111, 121, 128,   1,   4,   5,
          8,  10,  13,  26,  28,  30,  31,  32,  33,  35,  66,  73,  76,  84,
         87,  89,  98, 112, 126, 127, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([35, 4096])
🟦 gate.shape=torch.Size([35, 6400])
🟦 up.shape=torch.Size([35, 6400])
🟦 current_hidden_states.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([35, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  72,  95, 107,  81], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([117, 118, 122, 125,   0,  46,  47,  77, 119, 121, 124],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 4, 14, 34, 17, 56, 70], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  18,  35,  69, 101, 116,   3, 100, 102, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([115, 119, 126,  18, 116, 120, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,  19,  23,  24,  25,  37,  38,  40,  41,  42,  43,  50,  52,  53,
         54,  55,  57,  61,  62,  64,  65,  67,  71,  74,  80,  82,  83,  85,
         86,  90,  91,  92,  93,  94,  96,  97,  99, 103, 104, 105, 106, 108,
        109], device='cuda:0')
🟦 current_state.shape=torch.Size([43, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([43, 4096])
🟦 gate.shape=torch.Size([43, 6400])
🟦 up.shape=torch.Size([43, 6400])
🟦 current_hidden_states.shape=torch.Size([43, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([43, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  17,  36,  44,  14,  60, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   3,   5,   8,  10,  11,  26,  28,  30,  45,  47,  48,  58,  75,
         76,  77,  78,  88, 110, 112, 127, 129,   7,   9,  12,  15,  27,  29,
        111, 113, 117, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([  2, 118], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([ 69, 101], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 81, 120, 123, 124,  39,  48,  51,  58,  63,  72,  78,  88,  95, 107],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 45,  75, 110, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 1], device='cuda:0'), top_x=tensor([120, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  11,  12,  15,  29,  35, 113,   1,   2,   4,  14,  34,  38,  50,
         62], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  21,  26,  30,  32,  33,  39,  51,  63,  69,  70,  72,  81,  95,
        101, 102, 107, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 56,  91,  92,   0,  37,  59,  60,  67,  71,  74,  99, 105, 106, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
       device='cuda:0'), top_x=tensor([ 17,  20,  25,  38,  43,  50,  55,  57,  62,  67,  71,  74,  80,  85,
         86,  87,  94,  99, 104, 106, 109, 126,  16], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   2,   5,   9,  10,  27,  28,  58,  76,  88, 111, 112, 117, 124,
        128,   3,   8,  11,  45,  46,  47,  75,  77,  78, 110, 118, 121, 123,
        129], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 23,  41,  46,  97, 121,  24,  40,  42,  43,  54,  65,  66,  80,  83,
         84,  85,  91,  93,  94,  98, 104], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   4,  33,  70, 102, 116,   5,  18,  44,  56], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 47,  77, 118, 119, 122, 125,  48, 117, 120], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,  22,   7,  19,  20,  25,  55,  79, 119, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 44,  68,  69, 100, 101, 114,  13,  15,  35,  36,  89, 115],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,  53,  65,  83,  12,  23,  41,  49,  52,  57,  61,  64,  73,  76,
         82,  86,  90,  96, 103, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  31,  34,   6,  17,  29,  87, 126, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1], device='cuda:0'), top_x=tensor([ 19,  24,  37,  40,  42,  49,  52,  54,  59,  61,  64,  66,  73,  79,
         82,  84,  90,  93,  96,  98, 103, 105, 108,  22,  53,  97],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  16,  32,  36,  60,  89, 115,  68,  92, 100, 114],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,  21,  26,  30,  39,  45,  48,  51,  63,  72,  75,  78,  81,  95,
        107, 110, 127, 129,   9,  10,  27,  28,  31,  58,  88, 111, 112, 113,
        128], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0], device='cuda:0'), top_x=tensor([  0,  46, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  72,  81,  95, 107,  46,  69,  70,  75,  78, 101,
        102], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([ 18,  33,  69,  70, 101, 102, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  34,  14,  16, 115, 116, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   2,   3,  10,  47,  48,  77,  78,  88,   5,   7,   8,   9,  11,
         28,  58, 112, 113, 118, 124, 125, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,   7,  29,  45, 100, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 44,  68,  92, 100,  91, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 26,  27,  28,  30,  31,  45,  58,  66,  75,  76,  98, 110, 111, 112,
        127, 128, 129,  47,  48,  77,  84,  88, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 19,  20,  22,  23,  24,  25,  37,  38,  41,  42,  43,  49,  50,  53,
         54,  55,  56,  57,  59,  60,  61,  62,  65,  67,  71,  73,  74,  79,
         80,  83,  84,  85,  86,  87,  90,  91,  93,  94,  97,  99, 103, 104,
        105, 106, 108, 109,  17,  21,  36,  39,  40,  44,  51,  52,  63,  64,
         66,  68,  72,  81,  82,  89,  92,  95,  96,  98, 107, 122],
       device='cuda:0')
🟦 current_state.shape=torch.Size([68, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([68, 4096])
🟦 gate.shape=torch.Size([68, 6400])
🟦 up.shape=torch.Size([68, 6400])
🟦 current_hidden_states.shape=torch.Size([68, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([68, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 120, 122, 123, 124, 125, 126,  18,  20, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,  25,  38,  43,  50,  55,  56,  57,  62,  67,  71,  74,  80,  85,
         86,  87,  94,  99, 104, 109, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  4,   5,   8,   9,  11,  12, 113,   1,   2,   3,  10,  13,  26,  27,
         31,  32,  33,  76, 111, 127, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 40,  52,  64,  82,  96, 117, 118,  19,  22,  23,  24,  29,  30,  37,
         41,  42,  49,  53,  54,  59,  61,  65,  73,  79,  83,  90,  93,  97,
        103, 105, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([31, 4096])
🟦 gate.shape=torch.Size([31, 6400])
🟦 up.shape=torch.Size([31, 6400])
🟦 current_hidden_states.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([31, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  14,  16,  36,  89, 115,   4,  15,  34,  35,  60, 114],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 15,  32,  35, 114,  12], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([  0, 120, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 20,  38,  40,  52,  62,  64,  71,  74, 109,  19,  23,  24,  25,  36,
         37,  41,  42,  43,  49,  50,  53,  54,  55,  57,  60,  61,  65,  67,
         79,  80,  82,  83,  85,  86,  87,  89,  91,  92,  93,  94,  96,  97,
         99, 104, 105, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([46, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([46, 4096])
🟦 gate.shape=torch.Size([46, 6400])
🟦 up.shape=torch.Size([46, 6400])
🟦 current_hidden_states.shape=torch.Size([46, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([46, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 1], device='cuda:0'), top_x=tensor([ 4, 16], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  73, 121,  90, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([ 81, 117, 118, 119, 120, 122, 123, 124, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  17,  21,  36,  44,  51,  56,  57,  59,  60,  63,  68,  69,  70,
         86,  87,  89,  90,  91,  92, 100, 101, 102, 106, 115,  18,  22,  34,
         39,  48,  72,  81,  95, 103, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([35, 4096])
🟦 gate.shape=torch.Size([35, 6400])
🟦 up.shape=torch.Size([35, 6400])
🟦 current_hidden_states.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([35, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 26,  39,  72,  88,  95, 107, 127,  21,  45,  51,  58,  63,  68,  69,
         75, 100, 101, 110, 123, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 1], device='cuda:0'), top_x=tensor([ 2, 12,  3], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 19,  22,  23,  24,  25,  37,  41,  42,  43,  49,  50,  53,  54,  55,
         61,  65,  67,  79,  80,  83,  84,  85,  93,  94,  96,  97,  99, 103,
        104, 108,   0,  20,  38,  40,  46,  52,  56,  62,  64,  73,  74, 109,
        121, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([45, 4096])
🟦 gate.shape=torch.Size([45, 6400])
🟦 up.shape=torch.Size([45, 6400])
🟦 current_hidden_states.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([45, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0], device='cuda:0'), top_x=tensor([82], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  6,   7, 105, 126,  71, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  14, 114,  15,  32,  33,  35, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 1], device='cuda:0'), top_x=tensor([  3, 116, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  8,   9,  10,  11,  15,  18,  27,  28,  30,  31,  32,  33,  34,  35,
         45,  47,  48,  58,  75,  76,  77,  78, 110, 111, 112, 113, 128, 129,
          1,   2,   5,  12,  13,  14,  26,  29,  88,  98, 115, 116, 117, 118,
        127], device='cuda:0')
🟦 current_state.shape=torch.Size([43, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([43, 4096])
🟦 gate.shape=torch.Size([43, 6400])
🟦 up.shape=torch.Size([43, 6400])
🟦 current_hidden_states.shape=torch.Size([43, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([43, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 4, 17, 44, 70], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 29,  66,  30,  77,  78, 102, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   5,  98,   6,   7,   8,   9,  10,  11,  27,  28,  31,  47,  59,
         66,  76,  84, 111, 112, 120, 128, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,   9,  27,  28,  46,  66,  76,  84, 111, 121, 128,  10,  11,  30,
         31,  45,  75,  98, 110, 123, 124, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1, 118,   2,   3,   4,   5,  13,  47,  58,  81, 117],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([115, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([ 29,  30,  31,  45,  47,  58,  75,  98, 110,   8,   9,  15,  26,  27,
         32,  33,  48,  76,  77,  78,  84,  88, 111, 112, 113, 127, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  69,  70,  72,  77,  78,  81,  88,  95, 101, 102,
        107, 114,  52,  64,  79,  82,  83,  85,  96, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([18], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([6, 7], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  20,  23,  24,  25,  37,  38,  41,  42,  43,  49,  50,  53,  54,
         55,  61,  62,  65,  67,  71,  73,  74,  80,  83,  85,  86,  87,  90,
         93,  94,  97,  99, 103, 104, 105, 106, 109,   0,  17,  22,  36,  40,
         46,  56,  57,  59,  60,  66,  68,  89,  91,  92,  95, 100, 107, 121],
       device='cuda:0')
🟦 current_state.shape=torch.Size([56, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([56, 4096])
🟦 gate.shape=torch.Size([56, 6400])
🟦 up.shape=torch.Size([56, 6400])
🟦 current_hidden_states.shape=torch.Size([56, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([56, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  29,  35, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 1], device='cuda:0'), top_x=tensor([126,  44], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 118, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 48, 112, 123, 124, 129,  12,  28, 120], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1], device='cuda:0'), top_x=tensor([  2,   3,   4,   5,   6,   7,   8,  10,  11,  12,  13,  14,  15,  18,
         26,  32,  33,  34,  35, 113, 115, 116, 117, 127,   1,  16, 126],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  17,  36,  44,  56,  57,  59,  60,  68,  89,  91,  92, 100,  19,
         20,  21,  23,  24,  25,  34,  38,  39,  41,  42,  43,  50,  51,  53,
         54,  55,  61,  62,  63,  65,  67,  69,  70,  71,  72,  73,  74,  80,
         86,  87,  90,  94,  97,  99, 101, 102, 103, 104, 105, 106, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([55, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([55, 4096])
🟦 gate.shape=torch.Size([55, 6400])
🟦 up.shape=torch.Size([55, 6400])
🟦 current_hidden_states.shape=torch.Size([55, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([55, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  40,  52,  64,  79,  82,  96, 108, 120, 122, 125,  37,  49,  93,
        119], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 16/16 の処理スキップ
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66,  89, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,   9,   1,   7,  10,  47, 111, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,  23,  24,  46,  66, 121,  19,  22,  25,  37,  41,  42,  43,  53,
         54,  65,  83,  86,  90,  97, 103], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1], device='cuda:0'), top_x=tensor([  4,   6,   7, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   5,  10,  28,  47,  48,  77, 112, 129,   2,   3,   9,  11,  12,
         15,  58,  76,  78,  84,  88, 117, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 26,  39,  45,  58,  75,  78,  88, 110, 127,  21,  27,  48,  51,  63,
         72,  77,  81,  95, 107, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  18,  19,  20,  21,  22,  25,  34,  36,  37,  38,  40,  41,  42,
         43,  44,  49,  50,  51,  53,  54,  55,  56,  57,  59,  60,  61,  62,
         63,  64,  65,  67,  68,  69,  70,  71,  72,  73,  74,  79,  80,  81,
         83,  85,  86,  87,  89,  90,  91,  92,  93,  94,  95,  96,  97,  99,
        100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 117, 119,  16,  23,
         24,  35,  39,  52,  75,  98, 110, 118], device='cuda:0')
🟦 current_state.shape=torch.Size([78, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([78, 4096])
🟦 gate.shape=torch.Size([78, 6400])
🟦 up.shape=torch.Size([78, 6400])
🟦 current_hidden_states.shape=torch.Size([78, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([78, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  2,   3,  11,  12,  13,  14,  15,  29,  30,  32,  33,  35, 113, 114,
        116,   8,  26,  31,  34,  45, 115, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([118, 122, 125, 120, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([120, 123, 124,  59,  71,  73,  74,  79,  82, 104, 105, 106, 108, 109,
        119, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 27,  31,  76,  84,  98, 111, 128,   4,   5,  13,  28,  30, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 52,  82,  14,  40,  49,  61,  64,  96, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 29,  32,  33, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([126,   6,  17,  20,  38,  50,  55,  56,  57,  60,  62,  67,  80,  85,
         87,  91,  92,  93,  94,  99], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 16, 115,  36,  44], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  68,  69,  70, 100, 101, 102, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  18,  35,  69, 101, 115,  51,  63,  89,  95, 114, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,   3,  32,  11,  12, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 81,  85,  52,  79,  80,  82,  96, 108, 122, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 89,  35,  60,  70,  90, 101, 102, 120, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  17,  34,  44,  67,  68,  99, 100, 126,  16,  20,  36,  38,  57,
         62,  94, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([104, 106,  71,  74,  91, 105, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1], device='cuda:0'), top_x=tensor([119, 122, 125, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1], device='cuda:0'), top_x=tensor([120, 123, 124, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  72,  95, 107, 114,  26,  45,  48,  69,  77,  78,
         81, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,   9,  10,  11,  13,  26,  27,  28,  29,  30,  31,  45,  47,  48,
         58,  75,  77,  78,  88,  98, 110, 112, 113, 127, 128, 129,  15,  72,
         76, 111], device='cuda:0')
🟦 current_state.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([30, 4096])
🟦 gate.shape=torch.Size([30, 6400])
🟦 up.shape=torch.Size([30, 6400])
🟦 current_hidden_states.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([30, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   5,  22,  23,  40,  41,  42,  49,  52,  53,  64,  65,  73,  79,
         82,  83,  84,  90,  96,  97, 103, 108, 117, 118,  24,  59,  61,  66,
         93], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46, 121,  23,  25,  41,  43,  53,  54,  55,  65,  83,  97, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 33,  70,  92, 102, 116,   4,  14,  17,  18,  32, 118],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  37,  61,  66,  76,  93, 105, 111,   0,  27,  28,  30,  31,  46,
         47,  49,  56,  58,  75,  86,  87,  98, 103, 110, 112, 121, 128, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 16,  20,  24,  25,  36,  38,  43,  50,  54,  55,  56,  57,  59,  60,
         62,  71,  74,  80,  86,  87,  91,  94, 109,   6,   7,  19,  21,  22,
         33,  34,  37,  39,  40,  42,  44,  67,  68,  73,  84,  85,  88,  92,
         99, 100, 104, 106, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([47, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([47, 4096])
🟦 gate.shape=torch.Size([47, 6400])
🟦 up.shape=torch.Size([47, 6400])
🟦 current_hidden_states.shape=torch.Size([47, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([47, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,   6,   7,  12,   1,   2,   3,   5,   8,   9,  10,  13,  29,  50,
         64, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 1/16 の処理スキップ
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,   3,   6, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  20,  22,  23,  24,  25,  37,  38,  40,  41,  42,  43,  49,  50,
         52,  53,  54,  55,  61,  62,  64,  65,  67,  71,  73,  74,  79,  80,
         82,  83,  85,  86,  87,  93,  94,  96,  97,  99, 103, 104, 105, 106,
        108, 109,  17,  21,  39,  51,  56,  57,  81, 120], device='cuda:0')
🟦 current_state.shape=torch.Size([52, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([52, 4096])
🟦 gate.shape=torch.Size([52, 6400])
🟦 up.shape=torch.Size([52, 6400])
🟦 current_hidden_states.shape=torch.Size([52, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([52, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  35,  39,  48,  51,  63,  69,  70,  72,  78,  81,  95, 101, 102,
        107,  18,  88,  92], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 52,  63,  64,  72,  95,  96, 105, 107, 108, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 26,  30,  45,  58,  75,  88, 110, 114, 127, 129,  27,  28,  29,  31,
         32,  35,  77,  98, 111, 112, 113, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46, 121,   5,   7,   8,   9,  10,  11,  12,  23,  42,  47,  66,
         76], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([119, 120, 122, 123, 124, 125, 126,   1, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 9/16 の処理スキップ
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  40,  41,  53,  65,  73,  97, 103, 117, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([123], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  15,
         27,  28,  29,  31,  32,  47,  76,  77,  84,  98, 111, 112, 113, 117,
        118, 128,  30,  33,  45,  48,  58,  75,  78,  79,  82,  83, 101, 110,
        114, 116, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([45, 4096])
🟦 gate.shape=torch.Size([45, 6400])
🟦 up.shape=torch.Size([45, 6400])
🟦 current_hidden_states.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([45, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  34,  36,  56,  57,  91,  19,  20,  24,  25,  37,  38,  43,  44,
         49,  50,  54,  55,  59,  60,  61,  62,  67,  71,  74,  80,  84,  85,
         86,  87,  93,  94,  99, 104, 106, 109, 115, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([38, 4096])
🟦 gate.shape=torch.Size([38, 6400])
🟦 up.shape=torch.Size([38, 6400])
🟦 current_hidden_states.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([38, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  17,  18,  33,  44,  68,  90,  92, 100,   4,  26,  34,  70,  89,
         91, 102, 118, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1], device='cuda:0'), top_x=tensor([66,  0, 46, 69], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 59,  60,  89, 115, 116,  13,  14,  15,  16,  36,  68,  90, 100],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([37, 40, 61, 62, 71, 93, 19, 20, 25, 38, 43, 44, 49, 50, 54, 55, 64, 79,
        96], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  16,  59,  91, 106,  60,  89,  94, 104], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 1], device='cuda:0'), top_x=tensor([ 2,  3, 66], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  4,   7,  90, 105, 118,  17,  22,  42,  53,  56,  59,  65,  73,  83,
         86,  97, 103, 108, 117, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,  19,  20,  22,  23,  24,  25,  38,  41,  42,  43,  50,  53,  54,
         55,  56,  65,  73,  80,  83,  86,  87,  94,  97,  99, 103, 104,   7,
         37,  40,  46,  51,  52,  57,  61,  67,  74,  82,  85,  90,  93, 101,
        105, 109, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([45, 4096])
🟦 gate.shape=torch.Size([45, 6400])
🟦 up.shape=torch.Size([45, 6400])
🟦 current_hidden_states.shape=torch.Size([45, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([45, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 39,  52,  63,  64,  79,  81,  82,  85,  95,  96, 108,  80,  84, 100],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 14,  17,  34,  36,  44,  57,  60,  67,  68,  74,  92, 100, 109, 115,
        126,  16,  62,  71,  87,  91,  99, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([119, 120, 122, 123, 124, 125,  39], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0], device='cuda:0'), top_x=tensor([  0,  46,  66, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  18,  21,  32,  33,  48,  51,  69,  70,  72,  78,  89, 101, 102,
        107,  13,  14,  31,  34,  35,  36,  45,  63,  81,  88,  92,  95, 114,
        116], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 35, 113, 114,  26,  33], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 29,   4,  12,  15,  32, 112, 113, 115, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 26,  30,  45,  58,  75,  84,  88,  98, 110, 111, 127,   8,   9,  27,
         28,  47,  68,  72,  76,  77,  78, 107, 120, 123, 124, 125, 126, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  49,   1,   3,   6,  10,  11,  23,  24,  41, 122],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   8,   9,  10,  11,  12,  27,  28,  31,  47,  76,  77, 112, 117,
        128, 129,   0,   2,   5,  29,  30,  48,  58,  75,  98, 110, 111, 118,
        127], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([116,  18,  21,  69,  70, 102], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([115,  16,  35,  36,  60,  89, 100], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  15,
         27,  28,  31,  47,  76, 112, 116, 118, 128,  58,  75,  84, 111, 117,
        129], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 26,  29,  30,  75, 110, 111, 113, 127, 129,   8,   9,  11,  27,  28,
         31,  45,  47,  76,  98, 112, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 119, 121, 122, 124, 125,  22,  40, 120, 123],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([4], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([34, 49, 72, 88], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([120,  52,  64,  79,  82,  83,  85,  96, 105, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 8/16 の処理スキップ
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,   3,   6,  13,  14, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 24,   0,   1,  10,  23,  25,  41,  42,  43,  46,  53,  55,  65,  66,
         97, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  32,  33,  68,  70,  92, 102], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  20,  25,  38,  43,  50,  54,  55,  56,  57,  62,  67,  71,  80,
         85,  86,  87,  91,  94,  99, 104, 109, 126,  19,  24,  37,  44,  61,
         74,  93, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([31, 4096])
🟦 gate.shape=torch.Size([31, 6400])
🟦 up.shape=torch.Size([31, 6400])
🟦 current_hidden_states.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([31, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  23,  37,  40,  41,  42,  49,  52,  53,  61,  64,  65,  73,
         79,  82,  83,  93,  96,  97, 103, 105, 108, 117,   5,  50,  51,  54,
         56,  59,  80,  81,  86,  90, 118, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([36, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([36, 4096])
🟦 gate.shape=torch.Size([36, 6400])
🟦 up.shape=torch.Size([36, 6400])
🟦 current_hidden_states.shape=torch.Size([36, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([36, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 14/16 の処理スキップ
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  16,  18,  32,  33,  34,  35,  36,  44,  45,  48,  58,  59,  60,
         68,  69,  70,  72,  74,  84,  88,  89,  90,  92,  98, 100, 101, 102,
        106, 107,   7,  12,  15,  17,  20,  21,  26,  29,  30,  38,  39,  57,
         62,  63,  67,  71,  73,  77,  78,  87,  91,  94,  95,  99, 103, 104,
        108, 109, 113, 114, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([61, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([61, 4096])
🟦 gate.shape=torch.Size([61, 6400])
🟦 up.shape=torch.Size([61, 6400])
🟦 current_hidden_states.shape=torch.Size([61, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([61, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  77,  78,  81,  95, 114, 123,  48,  69, 101, 107,
        110, 124, 126, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  32,  15,  31,  33,  34,  57,  87, 116, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  56,  85,  92,  17,  55,  71,  91, 104, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  40,  49,  52,  61,  64,  73,  79,  82,  93,  96, 103, 108,   5,
         19,  23,  37,  41,  42,  53,  54,  56,  59,  65,  72,  74,  76,  83,
         84,  86,  90,  97,  98, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([34, 4096])
🟦 gate.shape=torch.Size([34, 6400])
🟦 up.shape=torch.Size([34, 6400])
🟦 current_hidden_states.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([34, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6, 105,   7,  24,  66,  93,  94, 120, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,   3,   9], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  36,  39,  51,  68,  70,  81,  99, 100, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 7/16 の処理スキップ
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,   9,  26,  27,  28,  29,  30,  31,  45,  47,  58,  75,  76,  77,
         84,  98, 110, 111, 112, 127, 128, 129,  10,  38,  44,  48,  88, 117,
        118], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  17,  20,  25,  34,  36,  38,  43,  44,  55,  57,  62,  67,  68,
         71,  74,  80,  86,  87,  91,  94,  99, 100, 104, 106, 109,  50,  60,
         85, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([30, 4096])
🟦 gate.shape=torch.Size([30, 6400])
🟦 up.shape=torch.Size([30, 6400])
🟦 current_hidden_states.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([30, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   2,   3,   5,   7,  10,  13,  15,  18, 116, 117, 118, 119, 120,
        122, 123, 124, 125, 126,   4,   6,   8,  11,  12,  14,  16,  21,  27,
         32,  47, 111, 115, 127, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([34, 4096])
🟦 gate.shape=torch.Size([34, 6400])
🟦 up.shape=torch.Size([34, 6400])
🟦 current_hidden_states.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([34, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 1, 1], device='cuda:0'), top_x=tensor([115,  35, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  51,  63,  72,  78,  81,  88,  95, 107,  26,  30,  45,  58,
         69,  75,  77, 101, 102, 106, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 11,  12,  59,  90, 113,   1,   2,  13,  22,  28,  29,  40,  52,  64,
         73,  89,  96, 103, 108, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([ 19,  23,  24,  37,  41,  42,  50,  53,  54,  65,  83,  97,  20,  25,
         43,  49,  61,  62,  67,  79,  80,  82, 119, 122, 125],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 33,  35,  48,  60,  69,  70,  89, 101, 102, 114,  63,  78,  92,  95,
        113], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 33,  35, 115, 116,  15,  18,  21,  36,  48,  69,  70,  78, 101, 102],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,   7,  19,  20,  24,  25,  37,  38,  43,  49,  50,  55,  56,  57,
         61,  62,  67,  71,  74,  79,  80,  85,  86,  87,  93,  94,  99, 104,
        106, 109,  17,  23,  41,  53,  65,  83,  97, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([38, 4096])
🟦 gate.shape=torch.Size([38, 6400])
🟦 up.shape=torch.Size([38, 6400])
🟦 current_hidden_states.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([38, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([88, 98], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  29,  34, 113, 114,  11,  13,  27,  28,  30,  31,  32,  33, 112,
        115, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,  40,  42,  52,  54,  59,  64,  73,  82,  90,  96, 103, 105, 108,
        117,  19,  24,  49,  61,  66,  79,  84,  86,  91], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 22,  23,  41,  53,  65,  83,  97,   6,  20,  25,  37,  38,  40,  42,
         43,  50,  52,  54,  55,  64,  67,  71,  73,  75,  80,  82,  85,  93,
         94,  96, 103, 104, 105, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([34, 4096])
🟦 gate.shape=torch.Size([34, 6400])
🟦 up.shape=torch.Size([34, 6400])
🟦 current_hidden_states.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([34, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 58,  74, 106, 118, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 1,  2,  3,  4, 12, 39, 45, 62], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 120, 122, 123, 124, 125,  76,  90, 107, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  17,  18,  44,  60,  68,  89,  91,  92, 100, 126,   7,  22,  56,
         59, 119], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 63,  81,  95,   9,  10,  47,  51,  72,  77,  92, 120, 122, 123, 124,
        128, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([110, 111], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,   2,   8, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1], device='cuda:0'), top_x=tensor([16, 36, 26, 44], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   8,   9,  10,  11,  12,  13,  15,  21,  26,  27,  28,  30,  31,
         32,  39,  45,  47,  48,  51,  58,  69,  70,  72,  75,  76,  77,  78,
         84,  88,  98, 101, 102, 107, 110, 111, 112, 118, 127, 128, 129,   1,
          5,  14,  16,  29,  34,  35,  57,  60,  63,  68,  81,  87,  89,  95,
         99, 100, 113, 114, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([61, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([61, 4096])
🟦 gate.shape=torch.Size([61, 6400])
🟦 up.shape=torch.Size([61, 6400])
🟦 current_hidden_states.shape=torch.Size([61, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([61, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66,  84, 121, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 2/16 の処理スキップ
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([  0,   2,  46,  66, 121,   5], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  5,   6,  19,  31,  37,  40,  41,  42,  49,  52,  53,  54,  61,  64,
         65,  79,  82,  83,  93,  96,  97, 108, 117, 118, 119],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7, 126,  14,  17,  20,  25,  34,  38,  43,  57,  62,  67,  71,  74,
         87,  94,  99, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  17, 115,  13,  36,  44,  59,  68, 100], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([119, 122, 125, 120, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,   6,  11,  12,  22,  23,  24,  29,  50,  56,  73,  86,  90,  91,
        103, 104, 105, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   8,   9,  10,  47,  76,  77, 117, 118, 120, 123, 124, 128,   3,
         27,  28,  51,  78,  81, 111, 112, 122, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  20,  21,  22,  23,  24,  25,  37,  38,  40,  41,  42,  43,  48,
         49,  50,  51,  52,  53,  54,  55,  56,  57,  59,  60,  61,  62,  63,
         64,  65,  67,  68,  69,  70,  71,  72,  73,  74,  78,  79,  80,  81,
         82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,
         96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
         26,  32,  39,  45,  58,  75,  77, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([78, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([78, 4096])
🟦 gate.shape=torch.Size([78, 6400])
🟦 up.shape=torch.Size([78, 6400])
🟦 current_hidden_states.shape=torch.Size([78, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([78, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  30, 116, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  36,  39,  21,  35,  48,  60,  63,  69,  89,  95, 101, 114],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  33,  55,  70,  80,  85,  92, 102, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  12,  13,  14,  15,  26,  27,  28,  29,  30,  31,  32,  33,  34,
         35,  44,  45,  58,  75, 110, 111, 112, 113, 114, 116, 127, 129,   1,
          8,  10,  16,  47,  76,  88,  98, 115, 126, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([38, 4096])
🟦 gate.shape=torch.Size([38, 6400])
🟦 up.shape=torch.Size([38, 6400])
🟦 current_hidden_states.shape=torch.Size([38, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([38, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 15/16 の処理スキップ
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   2,   4,   7,   9,  72, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 44,   4,  29,  31,  34, 100], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 89,  90,  54,  56,  59,  60,  63,  69,  86,  91,  92,  93,  95, 103,
        104, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 3/16 の処理スキップ
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 4/16 の処理スキップ
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   6,   7,  19,  22,  23,  24,  37,  41,  42,  49,  52,  53,  65,
         79,  83,  97, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 17,  20,  25,  38,  43,  62,  67,  68,  71,  74,  87,  44,  94,  99,
        106, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([ 48,  51,  55,  78,  81,  85,  92, 102, 107,   3,  15,  18,  21,  28,
         30,  32,  33,  36,  45,  50,  70,  72, 101, 115, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  14,  16,  26,  35,  89, 113, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 57,  60,  63,  94,  95,  99, 100, 106, 109, 110, 119, 120, 122, 123,
        124, 125,  38,  58,  62,  67,  68,  71,  74,  75,  80,  88, 107, 127],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,  11,  12,  40,  61,  64], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 84,   5,   9,  10,  81,  82,  96, 108, 111, 118], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 1], device='cuda:0'), top_x=tensor([2, 0], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,
         16,  18,  21,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,
         39,  45,  47,  58,  75,  76,  77,  88,  98, 111, 112, 113, 114, 115,
        116, 117, 118, 126, 127, 128, 129,   2,  17,  20,  48,  57,  78, 110,
        119, 120, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([60, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([60, 4096])
🟦 gate.shape=torch.Size([60, 6400])
🟦 up.shape=torch.Size([60, 6400])
🟦 current_hidden_states.shape=torch.Size([60, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([60, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,  27,  39,  47,  51,  76,  77,  98, 112, 117, 128, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  23,  24,  37,  40,  41,  42,  49,  50,  52,  53,  54,  56,
         59,  61,  64,  65,  73,  79,  80,  82,  83,  86,  91,  93,  96,  97,
        103, 104, 105, 108,  25,  43,  55,  66,  84,  85,  87,  90, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([41, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([41, 4096])
🟦 gate.shape=torch.Size([41, 6400])
🟦 up.shape=torch.Size([41, 6400])
🟦 current_hidden_states.shape=torch.Size([41, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([41, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 69,  70,  72, 101,  46,  73, 102, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 1/16 の処理スキップ
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  23,  24,  37,  40,  41,  42,  49,  52,  53,  54,  56,  59,
         61,  64,  65,  73,  79,  82,  83,  86,  90,  91,  93,  96,  97, 103,
        104, 105, 108,  84,  87, 106, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([35, 4096])
🟦 gate.shape=torch.Size([35, 6400])
🟦 up.shape=torch.Size([35, 6400])
🟦 current_hidden_states.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([35, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([25, 43, 67, 74], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 4/16 の処理スキップ
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([ 14,  17,  20,  25,  34,  38,  43,  44,  57,  62,  67,  68,  71,  74,
         87,  94,  99, 100, 106, 109, 125, 126,   6,   7,  55,  80, 104],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  84,  19,  22,  24,  37,  40,  41,  42,  49,  52,  53,  54,  59,
         61,  64,  65,  66,  71,  73,  79,  82,  83,  86,  90,  93,  96,  97,
         98, 103, 105, 108, 118, 119, 120], device='cuda:0')
🟦 current_state.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([35, 4096])
🟦 gate.shape=torch.Size([35, 6400])
🟦 up.shape=torch.Size([35, 6400])
🟦 current_hidden_states.shape=torch.Size([35, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([35, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13,  32,   1,  12,  14,  15,  16,  29,  31,  99, 115, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([35, 36, 50, 51, 80, 81, 92, 18, 33, 38, 39, 48, 60, 62, 78, 89, 94, 95],
       device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,  56,  91], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2, 101, 122, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  8,  26,  27,  28,  45,  47,  58,  75,  76,  77,  98, 110, 111, 127,
        128,   0,   4,   9,  10,  30,  46,  57, 112, 121, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([117, 118, 119, 120, 122, 123, 124,  47,  77, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 20,  23,  70,  85, 102], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  1,   2,   3,   4,   6,   7,   9,  10,  12,  29,  31, 112,   5,   8,
         11,  13,  27,  32,  76, 111, 113, 117, 126, 127, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 11,  21,  30,  39,  72,  95, 107, 113, 129,  26,  28,  34,  45,  51,
         63,  69,  75,  81,  88, 110, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  16,  18,  33,  48,  55,  60,  63,  69,  70,  78,  85,  88,  89,
        101, 102, 114, 115, 116,   3,  17,  21,  35,  36,  44,  50,  58,  68,
         72,  92, 100, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,   2,   5,  73], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  9,  27,  47,  76,  77,  98, 111, 124, 128,  26,  45,  75, 110, 112,
        117, 118, 119, 120, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([9], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 12,  13,  15,  10,  14,  32,  33, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 5/16 の処理スキップ
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 59,  66,  84, 103, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([18], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 8/16 の処理スキップ
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 16,  17,  18,  20,  21,  22,  23,  25,  26,  30,  31,  32,  33,  35,
         36,  39,  42,  43,  45,  48,  50,  51,  54,  55,  56,  57,  58,  59,
         60,  61,  62,  63,  67,  68,  69,  70,  71,  72,  73,  74,  75,  78,
         79,  80,  81,  84,  85,  86,  87,  88,  89,  90,  91,  92,  94,  95,
         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 114,
          0,  19,  24,  28,  37,  38,  40,  41,  44,  46,  47,  49,  52,  53,
         64,  65,  77,  82,  83,  93,  96,  97,  98, 115, 116, 121],
       device='cuda:0')
🟦 current_state.shape=torch.Size([96, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([96, 4096])
🟦 gate.shape=torch.Size([96, 6400])
🟦 up.shape=torch.Size([96, 6400])
🟦 current_hidden_states.shape=torch.Size([96, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([96, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 10,  11,  14,  28,  29,  34, 112, 129,   8,  12,  13,  15,  30, 113,
        127], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  3,  21,  27,  48,  58,  60,  63,  69,  70,  72,  76,  88,  89,  90,
         92,  95, 101, 102, 107, 108, 111, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5, 115, 116, 117, 118, 119, 120, 122, 123, 125, 126, 127,   4,  11,
        124, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  44,   6,  17,  25,  43,  67,  68,  71,  74,  94,  99, 100, 106,
        109, 125, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 2,  3,  4,  8,  1,  7, 39, 51, 78, 81], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 1, 29, 31, 34, 35, 62], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,  19,  24,  37,  38,  40,  41,  49,  52,  53,  64,  65,  82,  83,
         93,  96,  97,  16,  20,  22,  23,  36,  42,  50,  54,  55,  56,  57,
         61,  79,  80,  85,  86,  87,  91, 104, 122], device='cuda:0')
🟦 current_state.shape=torch.Size([37, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([37, 4096])
🟦 gate.shape=torch.Size([37, 6400])
🟦 up.shape=torch.Size([37, 6400])
🟦 current_hidden_states.shape=torch.Size([37, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([37, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  33, 116,   3,   5,  18,  70,  92, 102, 113, 115],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 73,  90,  24,  42,  59, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([ 47, 122, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([ 14,  17,  20,  38,  62,  71,  74,  94,  99, 106, 109,  25,  37,  43,
         50,  55,  57,  65,  67,  80,  85,  87,  93,  97, 104, 126],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  25,  43,  44,  67,  68, 100,   4,  14,  17,  29,  33,  34,  36,
         38,  95,  99], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,   0,  23,  40,  41,  46,  53,  83,  96, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 12,  15,  35,  88,   1,   2,  48,  58,  68,  72,  74,  75, 100, 107,
        110, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([19], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  24,  37,  40,  41,  42,  49,  52,  54,  56,  59,  61,  64,
         79,  82,  84,  86,  91,  93,  96, 105,  31,  66,  73, 103],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  8,  10,  11,  13,  26,  27,  28,  29,  30,  31,  32,  39,  45,  47,
         48,  57,  58,  75,  77,  78, 110, 112, 113, 127, 129,   9,  12,  44,
         76,  88,  98, 111, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([33, 4096])
🟦 gate.shape=torch.Size([33, 6400])
🟦 up.shape=torch.Size([33, 6400])
🟦 current_hidden_states.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([33, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([117, 118, 119, 124, 120, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   2,   3,  18, 120, 122, 123, 125, 126,   6,   7,   8,  11,  13,
        116, 117, 118, 119, 124, 127, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([39, 51, 63, 81], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,   7,  21,  23,  50,  51,  53,  55,  63,  65,  69,  70,  72,  80,
         81,  83,  85,  87,  89,  92,  95,  97, 101, 102, 103, 104, 107, 108,
         20,  22,  49,  52,  54,  56,  60,  61,  62,  64,  71,  79,  82,  86,
         90,  91,  94, 105, 106, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([48, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([48, 4096])
🟦 gate.shape=torch.Size([48, 6400])
🟦 up.shape=torch.Size([48, 6400])
🟦 current_hidden_states.shape=torch.Size([48, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([48, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 34,  36,  60, 114, 115,  15,  16,  21,  28,  30,  32,  35,  69,  78,
         89, 101], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,   9,  46,  66,  76,  98, 111, 121, 128,  10,  26,  27,  45,  77,
         84, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   8,   9,  10,  26,  27,  28,  45,  47,  57,  58,  75,  76,  77,
         98, 110, 111, 112, 120, 123, 124, 125, 127, 128, 129,  29,  30,  32,
        113, 122, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([31, 4096])
🟦 gate.shape=torch.Size([31, 6400])
🟦 up.shape=torch.Size([31, 6400])
🟦 current_hidden_states.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([31, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([117,   8,   9,  10,  26,  27,  47,  76, 111, 120, 123, 124, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 13,  16,  18,  32,  36,  60,  69, 114, 115,  11,  21,  39,  48,  63,
         72,  89,  92,  95, 101, 107], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  3,   5,  33, 116,  70, 102], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 5/16 の処理スキップ
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,  37,  49,  53,  83,  97,  19,  22,  23,  24,  40,  41,  42,  50,
         52,  54,  55,  56,  61,  64,  65,  79,  80,  82,  84,  85,  86,  91,
         93,  96, 104, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  21,  22,  23,  24,  39,  40,  41,  42,  48,  50,  51,  52,  54,
         55,  56,  59,  61,  63,  64,  65,  70,  72,  73,  78,  79,  80,  81,
         82,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  95,  96, 101,
        102, 103, 104, 105, 107, 108,  16,  17,  18,  20,  25,  31,  36,  37,
         38,  43,  44,  45,  49,  53,  60,  62,  67,  68,  69,  71,  74,  77,
         83,  94,  97,  98,  99, 100, 106, 109, 114, 115, 117, 119],
       device='cuda:0')
🟦 current_state.shape=torch.Size([82, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([82, 4096])
🟦 gate.shape=torch.Size([82, 6400])
🟦 up.shape=torch.Size([82, 6400])
🟦 current_hidden_states.shape=torch.Size([82, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([82, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0], device='cuda:0'), top_x=tensor([4], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46, 121,  59,  66,  73, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([118, 119, 122,   0,  46,  90, 121, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([13], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 1, 1], device='cuda:0'), top_x=tensor([ 66,   5, 118], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,  12,  15,  35,   1,  58,  88, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
       device='cuda:0'), top_x=tensor([  7,  14,  17,  20,  25,  34,  38,  43,  44,  62,  67,  68,  71,  74,
         94,  99, 100, 106, 109, 126,   6,  87], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  29,  30,  31, 113,  14,  28,  34,  57,  75, 112, 127, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,   3,   4,   7,  12,  15,  33,  35,  51,  78,  81, 103, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([ 47,  77, 117, 118, 119, 120, 122, 123, 124, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  24,  37,  42,  49,  54,  56,  61,  73,  79,  82,  84,  86,
         90,  93,  96,  98, 104, 108,  40,  41,  52,  57,  59,  64,  65,  83,
         91,  97, 105], device='cuda:0')
🟦 current_state.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([31, 4096])
🟦 gate.shape=torch.Size([31, 6400])
🟦 up.shape=torch.Size([31, 6400])
🟦 current_hidden_states.shape=torch.Size([31, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([31, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1], device='cuda:0'), top_x=tensor([111,   9,  27, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 23,  40,  41,  52,  53,  64,  65,  66,  83,  97,   5,  22,  26,  42,
         76,  82,  96, 103], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([126,   3,  13,  15, 116, 117, 118, 119, 125, 127, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([ 13,  14,  16,  20,  25,  31,  36,  43,  59,  60,  62,  67,  68,  89,
         91,  92,  94,  99, 115,  11,  17,  32,  34,  38, 100],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,   2,   8,   9,  10,  11,  12,  26,  46, 112, 113, 127,   1,   7,
         21,  75,  77,  84, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  21,  70,  72,  78, 102,  16,  48,  69,  71,  80, 101, 107],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  17,  34,  44,  14,  30,  43,  74,  88, 106, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  3,   4,  33,  48,  50,  55,  63,  80,  85, 114, 116,   6,  36,  51,
         53,  70,  78,  92,  95, 102, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 45,  81,  95, 121,   0,  39,  46,  50,  63,  87,  89, 104, 120, 122,
        123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([ 38,  57,  58,  71,  74,  75,  87,  88, 100, 106, 109, 110,  25,  31,
         44,  45,  55,  60,  62,  68,  72,  85,  94,  99, 126],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 27,  76, 103, 105, 128,  10,  28,  47,  66,  98, 111],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,   6,  29,  12,  19,  20,  23,  24,  37,  49,  54,  56,  61,  73,
         79,  86,  90,  93, 108, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 15,  39,  51,  69, 101, 107,  18,  35,  67,  81, 114],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,  28,  30,  32,  35, 129,   2,   4,   8,  29,  33,  58, 112, 113],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 31, 113,   1, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   2,   3, 129,  12,  15,  75,  88, 110, 127], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 3/16 の処理スキップ
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,   9,  10,  21,  27,  42,  47,  51,  54,  76,  84, 106, 111, 117,
        119, 120, 123, 124, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1], device='cuda:0'), top_x=tensor([  6,   7,  17,  20,  25,  38,  43,  44,  57,  62,  67,  68,  71,  74,
         87,  94,  99, 100, 104, 106, 109, 125, 126,  14,  34],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([39, 60, 63, 70, 81, 95], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  10,  26,  27,  47,  76, 111, 112, 127, 128,   8,  45,  77],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,   5,  46,  73, 108, 121,  74, 105, 109, 122], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([41, 56, 17, 19, 22, 23, 37, 40, 49, 50, 52, 53, 55, 57, 61, 64, 71, 79,
        80, 82, 83, 85, 86, 87, 91, 93, 96, 97], device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  24,  37,  40,  42,  49,  52,  54,  59,  61,  64,  66,  79,
         82,  84,  86,  90,  91,  93,  96,  97,  98, 103, 105,  11,  65,  73],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  8,  11,  12,  14,  15,  28,  30,  34,  35,   2,   7,  13,  16,  29,
         31,  36,  48,  58,  69,  72,  78, 112, 114, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  21,  23,  33,  36,  39,  45,  48,  50,  51,  53,  55,  58,  60,
         63,  65,  69,  70,  72,  75,  77,  78,  80,  81,  83,  85,  88,  89,
         92,  95, 101, 102, 107, 110, 114, 115, 116, 117,   4,  18,  20,  24,
         25,  26,  28,  30,  32,  35,  38,  41,  43,  44,  59,  62,  67,  68,
         90,  94,  98,  99, 100, 103, 104, 108, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([65, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([65, 4096])
🟦 gate.shape=torch.Size([65, 6400])
🟦 up.shape=torch.Size([65, 6400])
🟦 current_hidden_states.shape=torch.Size([65, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([65, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([  3, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([119, 120, 122, 123, 124, 118, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  4,  13,  18,  29,  32, 118,   6,  33,  56,  89,  92, 101, 102, 107,
        115], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 12,  15,  35,   2,  13,  14,  44, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([ 66, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   4, 115, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  21,  36,  39,  51,  60,  63,  69,  70,  72,  81,  89,  92,  95,
        101, 102, 107, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([16], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,  19,  24,  37,  40,  42,  49,  52,  59,  64,  73,  79,  82,  86,
         90,  93,  96, 103, 105, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  18,  20,  21,  23,  25,  36,  39,  41,  48,  50,  51,  53,  55,
         60,  63,  65,  69,  70,  72,  78,  80,  81,  83,  85,  88,  89,  92,
         95,  97,  99, 101, 102, 104, 107, 109, 113, 114,  17,  22,  28,  30,
         33,  35,  43,  45,  54,  56,  58,  61,  62,  67,  68,  71,  74,  75,
         84,  91,  94,  98, 100, 106, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([63, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([63, 4096])
🟦 gate.shape=torch.Size([63, 6400])
🟦 up.shape=torch.Size([63, 6400])
🟦 current_hidden_states.shape=torch.Size([63, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([63, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([119, 125, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 23,  41,  50,  53,  55,  80,  83,  85, 104, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  2,   3,   4,   6,  13,  32,  33, 115, 116,   1,   5,   7,   8,  10,
         11,  12,  15,  18,  31, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 29,  87, 118, 122, 125, 126,   9,  20,  26,  27,  38,  47,  57,  77,
        111, 120, 127, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0], device='cuda:0'), top_x=tensor([14, 34], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  1,   7,  17,  31,  38,  43,  44,  62,  67,  68,  71,  74,  94, 100,
        106,   6,  25,  29,  34,  99, 113], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,   8,   9,  10,  26,  27,  28,  30,  45,  46,  47,  57,  58,  66,
         75,  76,  77,  98, 110, 111, 112, 121, 127, 128, 129,  48,  78,  87,
         88], device='cuda:0')
🟦 current_state.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([29, 4096])
🟦 gate.shape=torch.Size([29, 6400])
🟦 up.shape=torch.Size([29, 6400])
🟦 current_hidden_states.shape=torch.Size([29, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([29, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([117, 119, 120, 123, 124,   0,  32,  46,  76, 112, 118, 121],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  24,  37,  40,  42,  49,  52,  54,  56,  59,  61,  64,  73,
         79,  82,  84,  86,  90,  91,  93,  96, 103, 105, 108,  65,  97, 122],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  37,  40,  49,  52,  56,  59,  61,  64,  79,  82,  86,  91,
         93,  96, 103, 105,   5,  24,  27,  42,  54,  73,  83,  84,  90,  98,
        108, 111], device='cuda:0')
🟦 current_state.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([30, 4096])
🟦 gate.shape=torch.Size([30, 6400])
🟦 up.shape=torch.Size([30, 6400])
🟦 current_hidden_states.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([30, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0], device='cuda:0'), top_x=tensor([120, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([  2,  12,  15,  35,  48,  50,  55,  57,  58,  69,  70,  72,  75,  78,
         80,  85,  88,  89, 107, 110, 114,   1,  10,  21,  26,  28,  30,  34,
         38,  39,  43,  45,  51,  59,  60,  62,  63,  67,  68,  71,  81,  92,
         95,  99, 100, 101, 102, 104, 106, 109, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([51, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([51, 4096])
🟦 gate.shape=torch.Size([51, 6400])
🟦 up.shape=torch.Size([51, 6400])
🟦 current_hidden_states.shape=torch.Size([51, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([51, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 13, 115, 116, 117, 119,   2,   3,   4,   8,  11,  18,  32,  33, 118,
        125, 126, 127, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([18, 4096])
🟦 gate.shape=torch.Size([18, 6400])
🟦 up.shape=torch.Size([18, 6400])
🟦 current_hidden_states.shape=torch.Size([18, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([18, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 1, 31, 12, 15, 44, 74], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 21,  23,  39,  41,  51,  53,  60,  63,  65,  81,  83,  92,  95,  97,
        101, 102, 118,  50,  69,  70,  72,  80,  88,  89,  91,  94, 107, 119,
        123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([30, 4096])
🟦 gate.shape=torch.Size([30, 6400])
🟦 up.shape=torch.Size([30, 6400])
🟦 current_hidden_states.shape=torch.Size([30, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([30, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([1, 1, 1], device='cuda:0'), top_x=tensor([  9, 120, 122], device='cuda:0')
🟦 current_state.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([3, 4096])
🟦 gate.shape=torch.Size([3, 6400])
🟦 up.shape=torch.Size([3, 6400])
🟦 current_hidden_states.shape=torch.Size([3, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([3, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([ 7, 85], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 18,  32,  33,  36, 113,  13,  16,  17,  23,  25,  35,  41,  48,  53,
         55,  65,  78,  97, 114, 115, 116, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  0,   8,  10,  11,  26,  28,  29,  30,  45,  46,  66, 112, 121, 127,
        129,  31,  58,  75, 110, 113, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1], device='cuda:0'), top_x=tensor([  6,   7,  14,  16,  17,  20,  25,  34,  38,  43,  44,  62,  67,  68,
         71,  74,  87,  94,  99, 100, 104, 106, 109, 125, 126,  29,  36,  57],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 12/16 の処理スキップ
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  24,  42,  54,  73,  84,  90,  98, 108,  19,  22,  37,  40,  47,
         49,  52,  56,  61,  64,  76,  77,  79,  82,  86,  93,  96, 103, 105],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  9,  27,  47,  76,  77, 111, 122, 128,  20,  87], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 3,  4,  6, 14], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([ 3, 13], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([30, 51], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,   2,   3,   4,   6,   7,   8,   9,  10,  11,  12,  13,  26,  27,
         28,  29,  76, 111, 112, 127, 128, 129,   0,   5,  14,  32,  45,  46,
         47,  57,  84,  87,  98, 121], device='cuda:0')
🟦 current_state.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([34, 4096])
🟦 gate.shape=torch.Size([34, 6400])
🟦 up.shape=torch.Size([34, 6400])
🟦 current_hidden_states.shape=torch.Size([34, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([34, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 60,  89,   2,  15,  35,  58,  88,  95,  99, 100, 106, 107, 109],
       device='cuda:0')
🟦 current_state.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([13, 4096])
🟦 gate.shape=torch.Size([13, 6400])
🟦 up.shape=torch.Size([13, 6400])
🟦 current_hidden_states.shape=torch.Size([13, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([13, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([124,  68, 117, 118, 119, 120, 122, 123, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([ 73,  80, 108,  11,  19,  22,  23,  24,  37,  40,  49,  50,  52,  53,
         56,  61,  64,  79,  82,  83,  86,  90,  91,  93, 103, 104, 105],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  71, 115, 116, 117, 118, 119, 120, 122, 123, 125, 126,  18,  74,
        124, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([16, 4096])
🟦 gate.shape=torch.Size([16, 6400])
🟦 up.shape=torch.Size([16, 6400])
🟦 current_hidden_states.shape=torch.Size([16, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([16, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 19,  22,  23,  24,  37,  40,  41,  42,  49,  52,  53,  54,  56,  59,
         61,  64,  65,  79,  82,  83,  84,  86,  90,  91,  93,  96,  97,  98,
        103, 104, 105,  10,  66,  73,  80, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([36, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([36, 4096])
🟦 gate.shape=torch.Size([36, 6400])
🟦 up.shape=torch.Size([36, 6400])
🟦 current_hidden_states.shape=torch.Size([36, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([36, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([36, 39, 41, 43, 48, 55, 65, 67, 81, 92, 97], device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 15,  30,  31,  35,  58,  88, 113,   1,   8,  12,  17,  25,  28,  34,
         44,  75,  85, 110, 114], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 21,  39,  48,  51,  55,  63,  69,  70,  72,  78,  81,  85,  92,  95,
        101, 102, 107,  16,  33, 116], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
       device='cuda:0'), top_x=tensor([ 14,  16,  17,  20,  25,  34,  38,  43,  44,  62,  67,  68,  74,  94,
         99, 100, 106, 109,  71, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  46,  66, 121,  42,  54,  59,  76,  77,  78,  96],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  32,  33,  36, 114,   4,  21,  60,  63,  69,  70,  72,  89, 101,
        102, 113, 115], device='cuda:0')
🟦 current_state.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([17, 4096])
🟦 gate.shape=torch.Size([17, 6400])
🟦 up.shape=torch.Size([17, 6400])
🟦 current_hidden_states.shape=torch.Size([17, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([17, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 45,  47,  50,  57,  75,  77,  87, 110,   6,   7,   9,  20,  26,  27,
         29,  38,  62,  94, 111, 127, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([21, 4096])
🟦 gate.shape=torch.Size([21, 6400])
🟦 up.shape=torch.Size([21, 6400])
🟦 current_hidden_states.shape=torch.Size([21, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([21, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([ 31, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 1/16 の処理スキップ
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([120, 122, 123,   5,  14,  19,  21,  39,  48,  51,  63,  73,  78,  79,
         81,  92,  93,  95, 108, 112], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([1, 1], device='cuda:0'), top_x=tensor([127, 129], device='cuda:0')
🟦 current_state.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 gate.shape=torch.Size([2, 6400])
🟦 up.shape=torch.Size([2, 6400])
🟦 current_hidden_states.shape=torch.Size([2, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 33,   3,  15,  16,  32,  36,  44,  72,  89, 101, 102, 107, 114, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 5/16 の処理スキップ
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  18,  21,  23,  25,  32,  35,  36,  39,  41,  43,  44,  48,  51,
         53,  55,  58,  60,  63,  65,  67,  68,  69,  70,  71,  72,  74,  78,
         80,  81,  83,  85,  87,  88,  89,  92,  95,  97,  99, 100, 101, 102,
        104, 107, 109, 113, 114,  17,  20,  22,  26,  28,  29,  30,  31,  33,
         34,  37,  38,  40,  45,  47,  49,  50,  52,  56,  57,  61,  62,  75,
         77,  82,  91,  94, 106, 110], device='cuda:0')
🟦 current_state.shape=torch.Size([76, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([76, 4096])
🟦 gate.shape=torch.Size([76, 6400])
🟦 up.shape=torch.Size([76, 6400])
🟦 current_hidden_states.shape=torch.Size([76, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([76, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  0,  11,  19,  22,  37,  40,  49,  52,  56,  59,  61,  64,  66,  79,
         82,  86,  91,  93,  96,  98, 105,  23,  24,  27,  41,  42,  46,  53,
         54,  65,  83,  84,  97, 103, 111, 121, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([37, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([37, 4096])
🟦 gate.shape=torch.Size([37, 6400])
🟦 up.shape=torch.Size([37, 6400])
🟦 current_hidden_states.shape=torch.Size([37, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([37, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  7,  14,  17,  20,  29,  31,  34,  38,  50,  57,  62,  94, 106, 125,
        126,   0,   6,  25,  43,  55,  67,  68,  71,  74,  80,  85,  87,  99,
        100, 104, 109, 113, 122], device='cuda:0')
🟦 current_state.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([33, 4096])
🟦 gate.shape=torch.Size([33, 6400])
🟦 up.shape=torch.Size([33, 6400])
🟦 current_hidden_states.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([33, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1], device='cuda:0'), top_x=tensor([  8,   9,  10,  26,  27,  28,  30,  45,  46,  75,  76, 110, 111, 112,
        117, 119, 121, 124, 127, 128, 129,  58,  60,  88, 120, 123],
       device='cuda:0')
🟦 current_state.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([26, 4096])
🟦 gate.shape=torch.Size([26, 6400])
🟦 up.shape=torch.Size([26, 6400])
🟦 current_hidden_states.shape=torch.Size([26, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([26, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([ 1,  2, 12, 15,  7,  8, 35], device='cuda:0')
🟦 current_state.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([7, 4096])
🟦 gate.shape=torch.Size([7, 6400])
🟦 up.shape=torch.Size([7, 6400])
🟦 current_hidden_states.shape=torch.Size([7, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([7, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([], device='cuda:0', dtype=torch.int64), top_x=tensor([], device='cuda:0', dtype=torch.int64)
🟦 エキスパート 11/16 の処理スキップ
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([4], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  3,   4,   6,  13, 115, 116,   2,   9,  10,  12], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 47,  77, 118,  90], device='cuda:0')
🟦 current_state.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([4, 4096])
🟦 gate.shape=torch.Size([4, 6400])
🟦 up.shape=torch.Size([4, 6400])
🟦 current_hidden_states.shape=torch.Size([4, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([4, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  5,  24,  42,  54,  73,  84,  90, 103, 108,   1,  59,  64,  66,  76,
         86,  96,  98, 105, 117, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([20, 4096])
🟦 gate.shape=torch.Size([20, 6400])
🟦 up.shape=torch.Size([20, 6400])
🟦 current_hidden_states.shape=torch.Size([20, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([20, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  13,  18,  69,  70, 115, 118, 119, 125, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([10, 4096])
🟦 gate.shape=torch.Size([10, 6400])
🟦 up.shape=torch.Size([10, 6400])
🟦 current_hidden_states.shape=torch.Size([10, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([10, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1], device='cuda:0'), top_x=tensor([  0,   9,  10,  26,  45,  58,  75,  87, 116, 129,   4,   7,   8,  12,
         27,  28,  30,  48,  50,  57,  78,  88, 110, 111, 112, 127, 128],
       device='cuda:0')
🟦 current_state.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([27, 4096])
🟦 gate.shape=torch.Size([27, 6400])
🟦 up.shape=torch.Size([27, 6400])
🟦 current_hidden_states.shape=torch.Size([27, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([27, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 19,  24,  37,  42,  49,  54,  59,  61,  79,  84,  86,  93,  98, 105,
         22,  40,  52,  64,  73,  82,  90,  96, 103, 108], device='cuda:0')
🟦 current_state.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([24, 4096])
🟦 gate.shape=torch.Size([24, 6400])
🟦 up.shape=torch.Size([24, 6400])
🟦 current_hidden_states.shape=torch.Size([24, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([24, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 31,  73, 103, 108,  24,  42,  54,  59,  66,  87,  98],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 16,  18,  21,  36,  39,  55,  60,  63,  69,  70,  72,  81,  85,  89,
         92,  95, 101, 102, 104, 107, 109, 114,  23,  33,  41,  51,  80,  99],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), top_x=tensor([ 46,  47,  76,  77, 117, 118, 119, 120, 121, 123, 124],
       device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([ 27,  66, 111, 128,   0,   5], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 33, 113, 115,   3,  11,  13,  14,  15,  16,  18,  21,  32,  35,  36,
         39,  44,  70,  95, 102, 107, 116, 125], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 65,  83,  97, 122,  53, 120, 121, 123], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,  29,  47, 122, 126], device='cuda:0')
🟦 current_state.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([5, 4096])
🟦 gate.shape=torch.Size([5, 6400])
🟦 up.shape=torch.Size([5, 6400])
🟦 current_hidden_states.shape=torch.Size([5, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([5, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1], device='cuda:0'), top_x=tensor([  3,   4,   5, 125,   6,  29], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 22,  23,  40,  41,  52,  53,  56,  64,  71,  82,  91,  96,  19,  31,
         61,  65,  83,  89,  97, 104, 105, 106], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
       device='cuda:0'), top_x=tensor([  6,   7,  14,  17,  20,  25,  34,  38,  43,  44,  62,  67,  68,  74,
         94,  99, 100, 106, 126,   2,  71, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([22, 4096])
🟦 gate.shape=torch.Size([22, 6400])
🟦 up.shape=torch.Size([22, 6400])
🟦 current_hidden_states.shape=torch.Size([22, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([22, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  2,  11,  13,  32,  34,  60,  69,  72, 101, 113, 114, 115],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 12,  15,  35,   1,  46,  76, 117, 118], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 90,  17,  20,  25,  37,  38,  43,  49,  56,  62,  63,  67,  68,  74,
         79,  84,  86,  91,  92,  93,  94, 100, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  8,  28,  30,  48,  50,  51,  57,  78,  80,  88, 110, 112, 127,   9,
         10,  26,  45,  55,  58,  75,  77,  81,  85, 119, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False output_router_logits=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096]) attention_mask.shape if attention_mask is not None else None=None position_ids=tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
         126, 127, 128, 129]], device='cuda:0') past_key_values is not None=True output_attentions=False use_cache=True cache_position=tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129], device='cuda:0') position_embeddings is not None=True
🟦 query_states.shape=torch.Size([1, 130, 4096])
🟦 key_states.shape=torch.Size([1, 130, 1024])
🟦 value_states.shape=torch.Size([1, 130, 1024])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 8, 130, 128])
🟦 value_states.shape=torch.Size([1, 8, 130, 128])
🟦 query_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 key_states.shape=torch.Size([1, 32, 130, 128])
🟩 repeat_kvを開始 hidden_states.shape=torch.Size([1, 8, 130, 128]) n_rep=4
🟦 hidden_states.shape=torch.Size([1, 8, 4, 130, 128])
🟩 repeat_kvを完了 res.shape=torch.Size([1, 32, 130, 128])
🟦 value_states.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 32, 130, 128])
🟦 attn_output.shape=torch.Size([1, 130, 32, 128])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟦 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSdpaAttentionの順伝播完了 attn_output.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 130, 4096])
🟦 hidden_states.shape=torch.Size([130, 4096])
🟦 router_logits.shape=torch.Size([130, 16])
🟩 sparsemixerを開始 scores.shape=torch.Size([130, 16]) jitter_eps=0.01 training=False top_k=2
🟩 sparsemixerを完了 multiplier.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 routing_weights.shape=torch.Size([130, 2]) selected_experts.shape=torch.Size([130, 2])
🟦 final_hidden_states.shape=torch.Size([130, 4096])
🟦 expert_mask.shape=torch.Size([16, 2, 130])
🟦 エキスパート 1/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 12,  15,  35,   1,   2,   8,  10,  17,  26,  30,  31,  33,  44, 114,
        126], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 1/16 の処理完了
🟦 エキスパート 2/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1], device='cuda:0'), top_x=tensor([  0,   3,   4,   6,   8,  10,  26,  28,  30,  45,  48,  50,  51,  58,
         75,  78,  88, 110, 112, 127, 129,  57,  76,  81, 116],
       device='cuda:0')
🟦 current_state.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([25, 4096])
🟦 gate.shape=torch.Size([25, 6400])
🟦 up.shape=torch.Size([25, 6400])
🟦 current_hidden_states.shape=torch.Size([25, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([25, 4096])
🟦 エキスパート 2/16 の処理完了
🟦 エキスパート 3/16 の処理開始
🟦 idx=tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  5,  19,  22,  24,  37,  40,  41,  42,  49,  52,  54,  56,  59,  61,
         64,  65,  66,  73,  79,  82,  83,  84,  86,  90,  93,  96,  97,  98,
        103, 105, 108, 117], device='cuda:0')
🟦 current_state.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([32, 4096])
🟦 gate.shape=torch.Size([32, 6400])
🟦 up.shape=torch.Size([32, 6400])
🟦 current_hidden_states.shape=torch.Size([32, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([32, 4096])
🟦 エキスパート 3/16 の処理完了
🟦 エキスパート 4/16 の処理開始
🟦 idx=tensor([1], device='cuda:0'), top_x=tensor([0], device='cuda:0')
🟦 current_state.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 gate.shape=torch.Size([1, 6400])
🟦 up.shape=torch.Size([1, 6400])
🟦 current_hidden_states.shape=torch.Size([1, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパート 4/16 の処理完了
🟦 エキスパート 5/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 19,  22,  24,  37,  40,  41,  42,  49,  52,  53,  54,  56,  59,  61,
         64,  65,  66,  73,  79,  82,  83,  84,  86,  90,  91,  93,  96,  97,
         98, 103, 105, 108,  23], device='cuda:0')
🟦 current_state.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([33, 4096])
🟦 gate.shape=torch.Size([33, 6400])
🟦 up.shape=torch.Size([33, 6400])
🟦 current_hidden_states.shape=torch.Size([33, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([33, 4096])
🟦 エキスパート 5/16 の処理完了
🟦 エキスパート 6/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 18,  21,  39,  55,  81,  85,  92,   3,  11,  16,  28,  36,  48,  50,
         51,  60,  63,  69,  70,  72,  78,  80,  95, 101, 102, 107, 112, 129],
       device='cuda:0')
🟦 current_state.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([28, 4096])
🟦 gate.shape=torch.Size([28, 6400])
🟦 up.shape=torch.Size([28, 6400])
🟦 current_hidden_states.shape=torch.Size([28, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([28, 4096])
🟦 エキスパート 6/16 の処理完了
🟦 エキスパート 7/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0'), top_x=tensor([ 63,  70,  89,  95, 101, 102, 107,  27,  58,  75,  88,  91,  92,  94,
        100, 104, 106, 111, 128], device='cuda:0')
🟦 current_state.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([19, 4096])
🟦 gate.shape=torch.Size([19, 6400])
🟦 up.shape=torch.Size([19, 6400])
🟦 current_hidden_states.shape=torch.Size([19, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([19, 4096])
🟦 エキスパート 7/16 の処理完了
🟦 エキスパート 8/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],
       device='cuda:0'), top_x=tensor([  7,  17,  20,  25,  29,  38,  43,  44,  57,  62,  67,  68,  71,  74,
         87,  94,  99, 100, 106, 109, 126,  14,  34], device='cuda:0')
🟦 current_state.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([23, 4096])
🟦 gate.shape=torch.Size([23, 6400])
🟦 up.shape=torch.Size([23, 6400])
🟦 current_hidden_states.shape=torch.Size([23, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([23, 4096])
🟦 エキスパート 8/16 の処理完了
🟦 エキスパート 9/16 の処理開始
🟦 idx=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 32,  39,  77, 118, 119, 120, 121, 123, 124], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 9/16 の処理完了
🟦 エキスパート 10/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 11,  13,  14,  16,  32,  34,  69,  72, 114,  18,  21,  29, 113, 115],
       device='cuda:0')
🟦 current_state.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([14, 4096])
🟦 gate.shape=torch.Size([14, 6400])
🟦 up.shape=torch.Size([14, 6400])
🟦 current_hidden_states.shape=torch.Size([14, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([14, 4096])
🟦 エキスパート 10/16 の処理完了
🟦 エキスパート 11/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  9,  46,  76, 117, 118, 119, 120, 121, 123, 124,   5,  47, 110, 122,
        125], device='cuda:0')
🟦 current_state.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([15, 4096])
🟦 gate.shape=torch.Size([15, 6400])
🟦 up.shape=torch.Size([15, 6400])
🟦 current_hidden_states.shape=torch.Size([15, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([15, 4096])
🟦 エキスパート 11/16 の処理完了
🟦 エキスパート 12/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 1], device='cuda:0'), top_x=tensor([ 33,  36, 115, 116, 125,  13], device='cuda:0')
🟦 current_state.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([6, 4096])
🟦 gate.shape=torch.Size([6, 6400])
🟦 up.shape=torch.Size([6, 6400])
🟦 current_hidden_states.shape=torch.Size([6, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([6, 4096])
🟦 エキスパート 12/16 の処理完了
🟦 エキスパート 13/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 23,  80, 104,   4,  53,  55,  71,  85, 109], device='cuda:0')
🟦 current_state.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([9, 4096])
🟦 gate.shape=torch.Size([9, 6400])
🟦 up.shape=torch.Size([9, 6400])
🟦 current_hidden_states.shape=torch.Size([9, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([9, 4096])
🟦 エキスパート 13/16 の処理完了
🟦 エキスパート 14/16 の処理開始
🟦 idx=tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 2, 60, 25, 38, 43, 62, 67, 68, 74, 89, 99], device='cuda:0')
🟦 current_state.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([11, 4096])
🟦 gate.shape=torch.Size([11, 6400])
🟦 up.shape=torch.Size([11, 6400])
🟦 current_hidden_states.shape=torch.Size([11, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([11, 4096])
🟦 エキスパート 14/16 の処理完了
🟦 エキスパート 15/16 の処理開始
🟦 idx=tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([ 27,  47,  77, 111, 122, 128,   6,   9,  20,  45,  87, 127],
       device='cuda:0')
🟦 current_state.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([12, 4096])
🟦 gate.shape=torch.Size([12, 6400])
🟦 up.shape=torch.Size([12, 6400])
🟦 current_hidden_states.shape=torch.Size([12, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([12, 4096])
🟦 エキスパート 15/16 の処理完了
🟦 エキスパート 16/16 の処理開始
🟦 idx=tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0'), top_x=tensor([  1,  31, 113,   7,  12,  15,  35,  46], device='cuda:0')
🟦 current_state.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([8, 4096])
🟦 gate.shape=torch.Size([8, 6400])
🟦 up.shape=torch.Size([8, 6400])
🟦 current_hidden_states.shape=torch.Size([8, 4096])
🟩 PhimoeBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([8, 4096])
🟦 エキスパート 16/16 の処理完了
🟦 final_hidden_states.shape=torch.Size([1, 130, 4096])
🟩 PhimoeSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 130, 4096]) router_logits.shape=torch.Size([130, 16])
🟩 PhimoeDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) self_attn_weights.shape if output_attentions else None=None router_logits.shape if output_router_logits else None=None
🟩 PhimoeModelの順伝播完了 hidden_states.shape=torch.Size([1, 130, 4096]) len(all_hidden_states) if all_hidden_states is not None else None=None len(all_self_attns) if all_self_attns is not None else None=None len(all_router_logits) if all_router_logits is not None else None=None
🟩 PhimoeForCausalLMの順伝播完了 logits.shape=torch.Size([1, 1, 32064]) loss.item() if loss is not None else None=None aux_loss.item() if aux_loss is not None else None=None